{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CyIlBR9ydLLT",
        "outputId": "4a596185-f37f-424f-ae99-3fec4337983b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting unsloth\n",
            "  Downloading unsloth-2025.5.9-py3-none-any.whl.metadata (47 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/47.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting unsloth_zoo>=2025.5.11 (from unsloth)\n",
            "  Downloading unsloth_zoo-2025.5.11-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: torch>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.6.0+cu124)\n",
            "Collecting xformers>=0.0.27.post2 (from unsloth)\n",
            "  Downloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Collecting bitsandbytes (from unsloth)\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: triton>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (3.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from unsloth) (24.2)\n",
            "Collecting tyro (from unsloth)\n",
            "  Downloading tyro-0.9.23-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3 (from unsloth)\n",
            "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting datasets>=3.4.1 (from unsloth)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: sentencepiece>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from unsloth) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from unsloth) (5.9.5)\n",
            "Requirement already satisfied: wheel>=0.42.0 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.45.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from unsloth) (2.0.2)\n",
            "Requirement already satisfied: accelerate>=0.34.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (1.7.0)\n",
            "Collecting trl!=0.15.0,!=0.9.0,!=0.9.1,!=0.9.2,!=0.9.3,>=0.7.9 (from unsloth)\n",
            "  Downloading trl-0.18.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft!=0.11.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.15.2)\n",
            "Collecting protobuf<4.0.0 (from unsloth)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.31.4)\n",
            "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.1.9)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.33.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from unsloth) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.1->unsloth) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=3.4.1->unsloth) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->unsloth) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.4.0->unsloth) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.4.0->unsloth) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers!=4.47.0,!=4.52.0,!=4.52.1,!=4.52.2,>=4.51.3->unsloth) (0.21.1)\n",
            "Collecting cut_cross_entropy (from unsloth_zoo>=2025.5.11->unsloth)\n",
            "  Downloading cut_cross_entropy-25.1.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from unsloth_zoo>=2025.5.11->unsloth) (11.2.1)\n",
            "Collecting msgspec (from unsloth_zoo>=2025.5.11->unsloth)\n",
            "  Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting torch>=2.4.0 (from unsloth)\n",
            "  Downloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting sympy>=1.13.3 (from torch>=2.4.0->unsloth)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12 (from nvidia-cudnn-cu12==9.1.0.70->torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.6.1.9->torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch>=2.4.0->unsloth)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton>=3.0.0 (from unsloth)\n",
            "  Downloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton>=3.0.0->unsloth) (75.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers->unsloth) (8.7.0)\n",
            "INFO: pip is looking at multiple versions of torchvision to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torchvision (from unsloth)\n",
            "  Downloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (13.9.4)\n",
            "Collecting shtab>=1.5.6 (from tyro->unsloth)\n",
            "  Downloading shtab-1.7.2-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typeguard>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from tyro->unsloth) (4.4.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=3.4.1->unsloth) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1.0->tyro->unsloth) (2.19.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers->unsloth) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.4.0->unsloth) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=3.4.1->unsloth) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.4.1->unsloth) (1.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro->unsloth) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.4.1->unsloth) (1.17.0)\n",
            "Downloading unsloth-2025.5.9-py3-none-any.whl (275 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.6/275.6 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m134.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.18.1-py3-none-any.whl (366 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading unsloth_zoo-2025.5.11-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.8/145.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xformers-0.0.30-cp311-cp311-manylinux_2_28_x86_64.whl (31.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.7.0-cp311-cp311-manylinux_2_28_x86_64.whl (865.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m133.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.0-cp311-cp311-manylinux_2_28_x86_64.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tyro-0.9.23-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.1/128.1 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shtab-1.7.2-py3-none-any.whl (14 kB)\n",
            "Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m128.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cut_cross_entropy-25.1.1-py3-none-any.whl (22 kB)\n",
            "Downloading msgspec-0.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (210 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, shtab, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, msgspec, fsspec, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, tyro, nvidia-cusolver-cu12, transformers, torch, datasets, xformers, torchvision, cut_cross_entropy, bitsandbytes, trl, unsloth_zoo, unsloth\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.52.2\n",
            "    Uninstalling transformers-4.52.2:\n",
            "      Successfully uninstalled transformers-4.52.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed bitsandbytes-0.46.0 cut_cross_entropy-25.1.1 datasets-3.6.0 fsspec-2025.3.0 msgspec-0.19.0 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 protobuf-3.20.3 shtab-1.7.2 sympy-1.14.0 torch-2.7.0 torchvision-0.22.0 transformers-4.52.4 triton-3.3.0 trl-0.18.1 tyro-0.9.23 unsloth-2025.5.9 unsloth_zoo-2025.5.11 xformers-0.0.30\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "e715b0943ba04810bc4db3a3facc54aa",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYp01xcysCxe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "if torch.cuda.is_bf16_supported():\n",
        "    dtype = torch.bfloat16\n",
        "else:\n",
        "    dtype = torch.float16\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289,
          "referenced_widgets": [
            "774051312b244179b362f2340d539a4b",
            "67ad394be5e0493c986902fdca8067e3",
            "cc0953e037c84e7d9ee7f0e1e3d3e489",
            "2bb2e2caf0064da3bb5524b29ea00131",
            "92e892581ded4cf187e6638012ec68ba",
            "3b18841d47e748b4be8384f56495ba4b",
            "d13c1b1a93a1455bb42dd6c244112c6d",
            "980716e0a3c04bd3943519aff7bb424f",
            "0fce6e3190444a8094720022a6d310a7",
            "7db5094af99f48e696a4bb7d06cb291c",
            "93add42250c944259857ee87d465a693"
          ]
        },
        "id": "mVCFKWe1cvLu",
        "outputId": "dbed06ef-c9ee-462d-c304-f1d6cbaf75ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Major: 8, Minor: 0\n",
            "==((====))==  Unsloth 2025.5.9: Fast Qwen3 patching. Transformers: 4.52.4.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-40GB. Num GPUs = 1. Max memory: 39.557 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "774051312b244179b362f2340d539a4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 2560])\n",
            "torch.Size([151936, 2560])\n",
            "Unsloth: Offloading output_embeddings to disk to save VRAM\n",
            "Unsloth: Training lm_head in mixed precision to save VRAM\n",
            "trainable parameters: 33037824\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup and imports\n",
        "# needed as this function doesn't like it when the lm_head has its size changed\n",
        "from unsloth import tokenizer_utils\n",
        "def do_nothing(*args, **kwargs):\n",
        "    pass\n",
        "tokenizer_utils.fix_untrained_tokens = do_nothing\n",
        "\n",
        "# Cell 2: Main imports and model loading\n",
        "import torch\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "print(f\"Major: {major_version}, Minor: {minor_version}\")\n",
        "from datasets import load_dataset\n",
        "import datasets\n",
        "from trl import SFTTrainer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from typing import Tuple\n",
        "import warnings\n",
        "from typing import Any, Dict, List, Union\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "NUM_CLASSES = 2 # YTA, NTA only\n",
        "\n",
        "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
        "# dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "\n",
        "model_name = \"unsloth/Qwen3-4B-Base\";load_in_4bit = False\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,load_in_4bit = load_in_4bit,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        ")\n",
        "# Explicitly move the model to the CUDA device after loading\n",
        "model.to(\"cuda\")\n",
        "\n",
        "# Cell 3: Trim classification head\n",
        "number_token_ids = []\n",
        "for i in range(0, NUM_CLASSES+1):\n",
        "    number_token_ids.append(tokenizer.encode(str(i), add_special_tokens=False)[0])\n",
        "# keep only the number tokens from lm_head\n",
        "par = torch.nn.Parameter(model.lm_head.weight[number_token_ids, :])\n",
        "\n",
        "old_shape = model.lm_head.weight.shape\n",
        "old_size = old_shape[0]\n",
        "print(par.shape)\n",
        "print(old_shape)\n",
        "\n",
        "model.lm_head.weight = par\n",
        "\n",
        "reverse_map = {value: idx for idx, value in enumerate(number_token_ids)}\n",
        "reverse_map\n",
        "\n",
        "# Cell 4: Setup PEFT model\n",
        "from peft import LoftQConfig\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\n",
        "        \"lm_head\", # can easily be trained because it now has a small size\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,  # We support rank stabilized LoRA\n",
        ")\n",
        "print(\"trainable parameters:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUSK60zXiJKP"
      },
      "outputs": [],
      "source": [
        "# Cell 5: Load and prepare dataset\n",
        "kaggle = os.getcwd() == \"/kaggle/working\"\n",
        "input_dir = \"/kaggle/input/amitheasshole/\" if kaggle else \"\"\n",
        "\n",
        "# Load the AITA dataset\n",
        "data = pd.read_csv(input_dir + \"amitheasshole.csv\", nrows = 10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOTGEwUhjzLZ",
        "outputId": "54242854-27f2-4da6-e56a-9cdd6dd3797b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 6)\n",
            "                                               title post_id  over_18  \\\n",
            "0        AITA for cutting communications with my ex?  b3jk0h    False   \n",
            "1  aita for thinking my girlfriend is dating me o...  b3jpqu    False   \n",
            "2             AITA For looking at my partners phone?  b3jsz3    False   \n",
            "3  AITA for taking an unvaccinated child to a fri...  b3k5l9    False   \n",
            "4  AITA when I give up on trying to follow the ru...  b3kbde    False   \n",
            "5       WIBTA for saying we should put down our dog?  b3kfm7    False   \n",
            "6  WIBTA for starting a relationship with a girl ...  b3ko1i    False   \n",
            "7       AITA for not wanting to give a wedding gift?  b3l0gb    False   \n",
            "8  WIBTA if I tell an awkward coworker he’s comin...  b3lgi3    False   \n",
            "9  WIBTA if I tell a guy I know his girlfriend is...  b3lube    False   \n",
            "\n",
            "       subreddit  link_flair_text  \\\n",
            "0  AmItheAsshole  No A-holes here   \n",
            "1  AmItheAsshole          Asshole   \n",
            "2  AmItheAsshole   Not the A-hole   \n",
            "3  AmItheAsshole          Asshole   \n",
            "4  AmItheAsshole   Not the A-hole   \n",
            "5  AmItheAsshole   Not the A-hole   \n",
            "6  AmItheAsshole          Asshole   \n",
            "7  AmItheAsshole   Not the A-hole   \n",
            "8  AmItheAsshole  No A-holes here   \n",
            "9  AmItheAsshole   Not the A-hole   \n",
            "\n",
            "                                           self_text  \n",
            "0  So me and my ex are both high school seniors, ...  \n",
            "1  so, hi. on mobile, second time poster, english...  \n",
            "2  Backstory: about 3 years ago my wife (fiancee ...  \n",
            "3  Ok so here’s the thing. My friends daughter is...  \n",
            "4  So. Lately, I've been extremely depressed. I'v...  \n",
            "5  Would it be wrong of me to recommend putting o...  \n",
            "6  I met this girl a few days ago and I can tell ...  \n",
            "7  I was taking to my partner about my niece’s we...  \n",
            "8  I have a coworker that I genuinely think is a ...  \n",
            "9  My girlfriend has a friend that is catfishing ...  \n"
          ]
        }
      ],
      "source": [
        "print(data.shape)\n",
        "print(data.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 879,
          "referenced_widgets": [
            "3d095bc2d571428d97f832ef17a1a4a2",
            "9156df23de8d4f3c84b171b58be4cba5",
            "1b6eb6008025446182410b733646f198",
            "456b458a44f14d82a50b4c452239865b",
            "e474e22744ad4f8db379078dee7d1ff8",
            "56e29dbc7e704b38980517c6834e7e91",
            "8e33c2d8e87b43a4b597c9d5b56aa782",
            "4bae24268b2a477dbb7fd6d72cea2c8f",
            "f02d2ab38b2040ba985f4d6faaed09e6",
            "bbf255a5e1874a78aa892a5d18f9ce1e",
            "8e247d05994f4cd7a2aee50de9d4d1d3"
          ]
        },
        "id": "37OKOOfniC-X",
        "outputId": "c3af22c6-0fe0-4b25-87e9-d11e7c3e6b11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution:\n",
            "link_flair_text\n",
            "Not the A-hole    2302\n",
            "Asshole            808\n",
            "Name: count, dtype: int64\n",
            "Before balancing:\n",
            "link_flair_text\n",
            "Not the A-hole    2302\n",
            "Asshole            808\n",
            "Name: count, dtype: int64\n",
            "Balancing to 808 samples per class\n",
            "After balancing:\n",
            "link_flair_text\n",
            "Asshole           808\n",
            "Not the A-hole    808\n",
            "Name: count, dtype: int64\n",
            "Total samples: 1616\n",
            "Training samples: 1454, Validation samples: 162\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/1454 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3d095bc2d571428d97f832ef17a1a4a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = NVIDIA A100-SXM4-40GB. Max memory = 39.557 GB.\n",
            "38.43 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create label mapping for AITA verdicts - BINARY ONLY\n",
        "label_mapping = {\n",
        "    'Asshole': 1,      # You're the A-hole\n",
        "    'Not the A-hole': 2,      # Not the A-hole\n",
        "}\n",
        "\n",
        "# Filter for posts with YTA or NTA labels only\n",
        "valid_labels = set(label_mapping.keys())\n",
        "data = data[data['link_flair_text'].isin(valid_labels)].copy()\n",
        "\n",
        "# Combine title and self_text for the full post content\n",
        "data['text'] = data['title'].fillna('') + '\\n\\n' + data['self_text'].fillna('')\n",
        "data['label'] = data['link_flair_text'].map(label_mapping)\n",
        "\n",
        "# Remove very short posts (less than 50 characters) and very long posts\n",
        "data = data[(data['text'].str.len() > 50) & (data['text'].str.len() < 10000)]\n",
        "\n",
        "# Sample if dataset is too large (optional)\n",
        "if len(data) > 30000:\n",
        "    data = data.sample(n=10000, random_state=42)\n",
        "\n",
        "# Balance the dataset if needed (optional)\n",
        "print(\"Label distribution:\")\n",
        "print(data['link_flair_text'].value_counts())\n",
        "\n",
        "\n",
        "print(\"Before balancing:\")\n",
        "print(data['link_flair_text'].value_counts())\n",
        "\n",
        "# ✅ BALANCE TO 50/50\n",
        "yta_data = data[data['label'] == 1]\n",
        "nta_data = data[data['label'] == 2]\n",
        "\n",
        "# Use the smaller class size to balance\n",
        "min_count = min(len(yta_data), len(nta_data))\n",
        "print(f\"Balancing to {min_count} samples per class\")\n",
        "\n",
        "yta_balanced = yta_data.sample(n=min_count, random_state=42)\n",
        "nta_balanced = nta_data.sample(n=min_count, random_state=42)\n",
        "\n",
        "# Combine and shuffle\n",
        "data = pd.concat([yta_balanced, nta_balanced])\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"After balancing:\")\n",
        "print(data['link_flair_text'].value_counts())\n",
        "print(f\"Total samples: {len(data)}\")\n",
        "\n",
        "# Continue with train/test split (this will maintain the 50/50 ratio)\n",
        "# train_df, val_df = train_test_split(data, test_size=0.1, random_state=42,stratify=data['label'])\n",
        "train_df, val_df = train_test_split(data[['text', 'label']], test_size=0.1, random_state=42, stratify=data['label'])\n",
        "print(f\"Training samples: {len(train_df)}, Validation samples: {len(val_df)}\")\n",
        "\n",
        "# Cell 6: Plot token distribution\n",
        "token_counts = [len(tokenizer.encode(x)) for x in train_df.text.sample(min(1000, len(train_df)))]\n",
        "plt.hist(token_counts, bins=30)\n",
        "plt.title(\"Token count distribution\")\n",
        "plt.show()\n",
        "\n",
        "# Cell 7: Setup prompt template and format data\n",
        "prompt = \"\"\"You are judging an AITA (Am I The Asshole) post from Reddit. Read the following post and determine if the person is the asshole or not.\n",
        "\n",
        "Post:\n",
        "{}\n",
        "\n",
        "Classify this post into one of the following categories:\n",
        "class 1: YTA (You're the A-hole) - The person writing the post is in the wrong\n",
        "class 2: NTA (Not the A-hole) - The person writing the post is not in the wrong\n",
        "\n",
        "SOLUTION\n",
        "The correct answer is: class {}\"\"\"\n",
        "\n",
        "def formatting_prompts_func(dataset_):\n",
        "    texts = []\n",
        "    for i in range(len(dataset_['text'])):\n",
        "        text_ = dataset_['text'].iloc[i]\n",
        "        label_ = dataset_['label'].iloc[i]\n",
        "\n",
        "        text = prompt.format(text_, label_)\n",
        "        texts.append(text)\n",
        "    return texts\n",
        "\n",
        "# apply formatting_prompts_func to train_df\n",
        "train_df['text'] = formatting_prompts_func(train_df)\n",
        "train_dataset = datasets.Dataset.from_pandas(train_df,preserve_index=False)\n",
        "# Cell 8: Custom data collator\n",
        "class DataCollatorForLastTokenLM(DataCollatorForLanguageModeling):\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer, # Pass tokenizer to the collator\n",
        "        *args,\n",
        "        mlm: bool = False,\n",
        "        ignore_index: int = -100,\n",
        "        reverse_map: dict = None, # Pass reverse_map\n",
        "        label_mapping: dict = None, # Pass original label_mapping\n",
        "        prompt_template: str = None, # Pass the prompt template\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(tokenizer, *args, mlm=mlm, **kwargs)\n",
        "        self.ignore_index = ignore_index\n",
        "        self.reverse_map = reverse_map\n",
        "        self.label_mapping = label_mapping\n",
        "        self.prompt_template = prompt_template\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
        "        # The base collator handles padding and creates input_ids, attention_mask, and labels\n",
        "        batch = super().torch_call(examples)\n",
        "\n",
        "        for i in range(len(examples)):\n",
        "            # Original full text before formatting\n",
        "            # Access the original example to get the true label before formatting\n",
        "            # This requires the original dataset structure to be available in examples,\n",
        "            # which `from_pandas` does provide if preserve_index=False (which it is).\n",
        "            # However, the example passed to collator is the output of formatting_prompts_func\n",
        "            # Let's assume the formatted text still implicitly contains the label information\n",
        "            # Or, we can rely on the labels generated by the base collator IF the label tokens are present\n",
        "\n",
        "            # We need to find the token ID for the specific label digit at the end of the prompt.\n",
        "            # The prompt ends with \"class {label_digit}\" where label_digit is '1' or '2'.\n",
        "            # We need the token ID for '1' and '2'. We already computed these.\n",
        "            label_token_ids = self.reverse_map.keys() # These are the tokens for '0', '1', '2'\n",
        "\n",
        "            labels = batch[\"labels\"][i]\n",
        "            attention_mask = batch[\"attention_mask\"][i]\n",
        "\n",
        "            # Find the end of the non-padded sequence\n",
        "            # last_non_padding_idx = (attention_mask.bool()).nonzero()[-1].item()\n",
        "\n",
        "            # Instead of relying on the last non-padding token,\n",
        "            # find the token ID corresponding to the label digit (\"1\" or \"2\")\n",
        "            # which should be near the end of the sequence.\n",
        "            # This is safer because the last token might not be the digit itself.\n",
        "            # We know the prompt ends with \"class 1\" or \"class 2\".\n",
        "            # Let's try to find the token for '1' or '2' near the end.\n",
        "\n",
        "            # Convert sequence to list for easier searching\n",
        "            label_sequence = labels.tolist()\n",
        "\n",
        "            found_label_token_idx = -1\n",
        "            true_label_digit = -1 # The actual digit '1' or '2'\n",
        "\n",
        "            # Iterate backwards from the end of the non-padded sequence\n",
        "            # to find the first occurrence of a label token ('1' or '2')\n",
        "            last_non_padding_idx = (attention_mask.bool()).sum().item() - 1\n",
        "\n",
        "            # Define the tokens for '1' and '2' dynamically from reverse_map\n",
        "            token_id_for_1 = None\n",
        "            token_id_for_2 = None\n",
        "            for original_token_id, new_index in self.reverse_map.items():\n",
        "                 # We need to know which original token corresponds to which digit\n",
        "                 # Let's re-encode the digits to be sure\n",
        "                 encoded_1 = self.tokenizer.encode(\"1\", add_special_tokens=False)[0]\n",
        "                 encoded_2 = self.tokenizer.encode(\"2\", add_special_tokens=False)[0]\n",
        "                 if original_token_id == encoded_1:\n",
        "                     token_id_for_1 = original_token_id\n",
        "                 if original_token_id == encoded_2:\n",
        "                     token_id_for_2 = original_token_id\n",
        "\n",
        "            # Search near the end for the specific label token ID (1 or 2)\n",
        "            # We search backwards from the last non-padding token\n",
        "            # Search within the last 10 tokens as a heuristic\n",
        "            search_start_idx = max(0, last_non_padding_idx - 10) # Look in the last 10 non-padding tokens\n",
        "\n",
        "            for idx in range(last_non_padding_idx, search_start_idx - 1, -1):\n",
        "                current_token_id = label_sequence[idx]\n",
        "                if current_token_id in [token_id_for_1, token_id_for_2]:\n",
        "                    found_label_token_idx = idx\n",
        "                    # We found the token id for the digit 1 or 2\n",
        "                    # This is the token we want to predict\n",
        "                    batch[\"labels\"][i, :found_label_token_idx] = self.ignore_index\n",
        "                    # Keep only the label for the found token\n",
        "                    batch[\"labels\"][i, found_label_token_idx + 1:] = self.ignore_index\n",
        "\n",
        "                    # Map the original token ID to the new lm_head index (0, 1, or 2)\n",
        "                    # Ensure the found_label_token_idx corresponds to a token we care about predicting ('1' or '2')\n",
        "                    if current_token_id in self.reverse_map:\n",
        "                        batch[\"labels\"][i, found_label_token_idx] = self.reverse_map[current_token_id]\n",
        "                    else:\n",
        "                         # This case should ideally not happen if the logic is correct,\n",
        "                         # but as a fallback, ignore this token if it's not in the map\n",
        "                         batch[\"labels\"][i, found_label_token_idx] = self.ignore_index\n",
        "\n",
        "                    break # Stop searching once the label token is found\n",
        "\n",
        "            # If the label token ('1' or '2') was not found near the end,\n",
        "            # this sample is problematic. We should set all labels to ignore_index.\n",
        "            if found_label_token_idx == -1:\n",
        "                print(f\"Warning: Could not find label token (1 or 2) near the end of sequence {i}.\")\n",
        "                # Optionally set all labels to ignore_index for this problematic sample\n",
        "                batch[\"labels\"][i, :] = self.ignore_index\n",
        "\n",
        "\n",
        "        return batch\n",
        "\n",
        "# Update the collator initialization to pass necessary arguments\n",
        "collator = DataCollatorForLastTokenLM(\n",
        "    tokenizer=tokenizer,\n",
        "    reverse_map=reverse_map,\n",
        "    label_mapping=label_mapping, # Pass label_mapping if needed, though reverse_map should be sufficient\n",
        "    prompt_template=prompt # Pass prompt template if needed for debugging/more complex logic\n",
        ")\n",
        "\n",
        "# Cell 9: Setup trainer (rest of the code remains the same)\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = train_dataset,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 1,\n",
        "    packing = False, # not needed because group_by_length is True\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 16,  # Adjust based on your GPU\n",
        "        gradient_accumulation_steps = 1,\n",
        "        warmup_steps = 10,\n",
        "        learning_rate = 1e-4,\n",
        "        fp16 = (dtype == torch.float16),\n",
        "        bf16 = (dtype == torch.bfloat16),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        num_train_epochs = 3,\n",
        "        report_to = \"none\",\n",
        "        group_by_length = True,\n",
        "        # max_steps=120,\n",
        "    ),\n",
        "    data_collator=collator,\n",
        "    dataset_text_field=\"text\",\n",
        ")\n",
        "\n",
        "    # args = TrainingArguments(\n",
        "    #     per_device_train_batch_size = 32,\n",
        "    #     gradient_accumulation_steps = 1,\n",
        "    #     warmup_steps = 10,\n",
        "    #     learning_rate = 1e-4,\n",
        "    #     fp16 = not torch.cuda.is_bf16_supported(),\n",
        "    #     bf16 = torch.cuda.is_bf16_supported(),\n",
        "    #     logging_steps = 1,\n",
        "    #     optim = \"adamw_8bit\",\n",
        "    #     weight_decay = 0.01,\n",
        "    #     lr_scheduler_type = \"cosine\",\n",
        "    #     seed = 3407,\n",
        "    #     output_dir = \"outputs\",\n",
        "    #     num_train_epochs = 1,\n",
        "    #     # report_to = \"wandb\",\n",
        "    #     report_to = \"none\",\n",
        "    #     group_by_length = True,\n",
        "    # ),\n",
        "\n",
        "# Cell 10: Show memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OTboHsdr-xvn",
        "outputId": "c7f0caad-56b8-406f-ea2f-20667bb7cc13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 1,454 | Num Epochs = 3 | Total steps = 273\n",
            "O^O/ \\_/ \\    Batch size per device = 16 | Gradient accumulation steps = 1\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (16 x 1 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 33,037,824/4,055,513,600 (0.81% trained)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not find label token (1 or 2) near the end of sequence 0.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 1.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 2.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 3.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='273' max='273' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [273/273 07:56, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.751200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.620900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.625900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.723900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.545600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.609900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.761000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.773000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.810500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.731800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.670300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.624600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.641900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.670100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.560500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.783200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.712300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.647000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.687000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.722800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.725500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.697500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.687000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.722200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.765200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.720700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.685600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.664200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.735800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.702500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.720400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.673900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.675700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.694600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.704000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.748300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.690900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.641100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.688700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.687100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.593300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.740500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.837700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.841700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.782200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.644400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.672600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.587100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.489800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.797000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.483700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.532100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.894100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.604300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.648400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.775100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.614100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.585300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.513900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.633200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.525100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.564700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.629300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.687200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.524400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.530400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.492400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.544500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.613300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.763600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.984600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.859000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.564200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.670700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.555900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.644000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.783700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.769400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.658900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.616700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.717100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.739300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.657500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.597300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.664300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.720600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.789900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.708700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.650300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.631600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.582300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.578000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.455600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.455100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.605900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.025200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.669900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.778300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>1.082300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.637900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.884200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.402800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.819000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.539900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.566600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.481200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.790100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.581400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.772200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.563200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.603200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.521300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.692400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.453300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.690300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.589600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.579200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.482400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.383800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.491300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.800400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.625200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.706200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.547700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.710300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.428700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.404200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.500300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.338500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.770000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.579000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.634000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.390600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.558600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.660200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.417100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.535000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.403100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.601200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.546000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.867000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.416300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.494200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.482100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.674200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.554200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.489900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.625100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.745200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.888000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.520300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.921300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.370600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.558300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>1.060000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.711400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.390100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.402700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.557000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.540700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.619000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.640800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.563900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.418500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.793800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.645500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.527800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.786000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.604500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.450400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.513700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.695800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.618800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.556900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.568200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.572000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.388500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.695900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.513300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.441300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.340000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.360700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.298200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.532000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.463400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.492800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.333100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.208900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.342300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.353700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.309100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.383700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.264700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.223700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.646500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.433400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.368700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.319900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.756100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.589800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.360700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.313100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.317200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.135200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.237000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.559700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.344900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.383200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.763900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.383600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.499100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.944800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>0.335100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.197500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.541300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.299100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.345100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.342200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.446800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.439700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.636300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.700300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.219700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.362300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.634400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.536200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.106800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.110500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.501600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.387000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.199700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.269400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.436500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.291500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.728100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.708200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.445700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.176500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.289800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.592400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.421800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.319300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.487000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.167300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.429300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>0.548600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.220000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.589500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.435200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.176600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.440100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.254300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.141400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.657000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.513500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.331300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.424700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.343600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.470000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.641600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.408200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.359900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.519400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.482500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.676100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.264000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.422300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.499600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Could not find label token (1 or 2) near the end of sequence 0.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 1.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 0.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 1.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 2.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 0.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 1.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 0.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 0.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 1.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 0.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 1.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 0.\n",
            "Warning: Could not find label token (1 or 2) near the end of sequence 0.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Cell 11: Train the model\n",
        "trainer_stats = trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUnboIrY-vqk",
        "outputId": "a7af8a53-528d-46c9-ec3f-d4aefa45827c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "480.4391 seconds used for training.\n",
            "8.01 minutes used for training.\n",
            "Peak reserved memory = 38.43 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 97.151 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n",
            "\n",
            "Remade lm_head: shape = torch.Size([151936, 2560]). Allowed tokens: [15, 16, 17]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 21/21 [00:05<00:00,  3.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation accuracy: 53.70% (87/162)\n",
            "\n",
            "--- Confusion Matrix ---\n",
            "YTA predicted as YTA: 72\n",
            "YTA predicted as NTA: 9\n",
            "NTA predicted as YTA: 66\n",
            "NTA predicted as NTA: 15\n",
            "\n",
            "--- Random samples ---\n",
            "\n",
            "Text: AITA for telling my gf that I need to study for my ACT?\n",
            "\n",
            "[deleted]...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.905, NTA: 0.095\n",
            "\n",
            "Text: AITA if I don't want to pool baby shower gifts with my mum?\n",
            "\n",
            "Hello reddit, just need a quick opinion to help me to decide whether I'm valid in what I'm feeling or whether I'm just being a prick.  My f...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.679, NTA: 0.321\n",
            "\n",
            "Text: AITA for stopping people from re-sharing the same story with me for the x'th time?\n",
            "\n",
            "Basically the title, I have some friends who have a habit of repeating the same story to me multiple times. I unders...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.753, NTA: 0.244\n",
            "\n",
            "Text: AITA Mac and Cheese\n",
            "\n",
            "So I’m a 16(M) on a band trip to San Antonio, we stop to eat at Panera bread, me being me, I make a joke while in line “I’m gonna order fucking mac and cheese and nobody can stop ...\n",
            "True: YTA  Pred: YTA ✅\n",
            "Confidence: YTA: 0.881, NTA: 0.119\n",
            "\n",
            "Text: AITA for telling my female friend that she could make her roleplay character \"boss people about\"?\n",
            "\n",
            "This happened a week ago.  I play a roleplaying game with a group of my friends, and I asked one of m...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.953, NTA: 0.047\n",
            "\n",
            "Text: AITA for giving my roommate a taste of his own medicine?\n",
            "\n",
            "Okay so I stay in a one room dorm with two other guys. For the most part, we are really good friends and hang out all the time, it's just that...\n",
            "True: YTA  Pred: NTA ❌\n",
            "Confidence: YTA: 0.029, NTA: 0.971\n",
            "\n",
            "Text: WIBTA for telling my son I know he's gay\n",
            "\n",
            "I never suspected my son was gay, he's never had any girlfriends but never seemed too feminine or anything and just a normal boy. But recently I discovered ga...\n",
            "True: YTA  Pred: YTA ✅\n",
            "Confidence: YTA: 0.592, NTA: 0.407\n",
            "\n",
            "Text: AITA for resenting my disabled brother\n",
            "\n",
            "I saw the story about the guy who cares for his sister and thought maybe this would be a good place to vent/get opinions TLDR at the end (Let me preface this wi...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.584, NTA: 0.414\n",
            "\n",
            "Text: AITA for calling my friend a “braindead c**t” over a movie?\n",
            "\n",
            "I (F19) have a “friend” (M25), who I will call Al, I put friend in quotation marks because if we’re being honest, he isn’t exactly my frien...\n",
            "True: YTA  Pred: YTA ✅\n",
            "Confidence: YTA: 0.678, NTA: 0.320\n",
            "\n",
            "Text: AITA for parking at a strip mall and then walking to church?\n",
            "\n",
            "In the past few weeks I've taken advantage of the nice weather and instead of parking in the parking lot at church do it at a strip mall a...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.692, NTA: 0.307\n",
            "\n",
            "Text: AITA for being upset about my laptop?\n",
            "\n",
            "So this is a pretty retroactive AITA, but it’s something that still kind of irritates me to this day, and my parents thought the situation was perfectly reasonab...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.480, NTA: 0.480\n",
            "\n",
            "Text: AITA for telling my cousin to mind her own business?\n",
            "\n",
            "So I (19f) am a professional drummer. I have been playing the drums for about 10 years and I've been playing in y band for roughly 3 years now.   ...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.816, NTA: 0.176\n",
            "\n",
            "Text: AITA for not letting my ex take our 3 week old son overnight?\n",
            "\n",
            "My ex (30M) and I (27F) broke up when I was 7 months pregnant with both of our first child. We’d only been together for a month when I fo...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.800, NTA: 0.193\n",
            "\n",
            "Text: AITA for startling an old woman at the grocery store?\n",
            "\n",
            "I am shopping for stuff at a grocery store while on the clock. The store rhymes with window.. anyways, the store is not super busy, but there’s d...\n",
            "True: YTA  Pred: YTA ✅\n",
            "Confidence: YTA: 0.731, NTA: 0.269\n",
            "\n",
            "Text: AITA-my parents put my dog down while I was at work knowing I wanted to be there and didn’t tell me bout it til I got off work\n",
            "\n",
            "We use to have 5 dogs total one for each family member.  Two died last y...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.743, NTA: 0.257\n",
            "\n",
            "Text: AITA for telling my friend's boyfriend to break up with her?\n",
            "\n",
            "My friend's been in a relationship for 2 years and for the last few months, she revealed to me how horrible her relationship is. Her boyfr...\n",
            "True: YTA  Pred: YTA ✅\n",
            "Confidence: YTA: 0.968, NTA: 0.032\n",
            "\n",
            "Text: WIBTA if I told my relative(F18)'s family about her secret relationship with her SO (M25)?\n",
            "\n",
            "[deleted]...\n",
            "True: NTA  Pred: NTA ✅\n",
            "Confidence: YTA: 0.407, NTA: 0.592\n",
            "\n",
            "Text: AITA for not wanting neighborhood kids to play on my porch?\n",
            "\n",
            "My Wife and I have twin boys who are 8 years old. We live in a nice / safe neighborhood that's pretty old-school in the sense that all the ...\n",
            "True: NTA  Pred: YTA ❌\n",
            "Confidence: YTA: 0.851, NTA: 0.148\n",
            "\n",
            "Text: AITA for smacking cars that park in crosswalks?\n",
            "\n",
            "When I'm a pedestrian approaching a stoplight and a car is parked in the crosswalk, I always slap the back of it a few times real loud as I'm walking b...\n",
            "True: YTA  Pred: YTA ✅\n",
            "Confidence: YTA: 0.907, NTA: 0.093\n",
            "\n",
            "Text: AITA for abusing induction loops at left turn lanes in traffic just to go first?\n",
            "\n",
            "So for those that aren't aware, left turn lanes in my area have induction loops embedded into the roads. There will be...\n",
            "True: YTA  Pred: YTA ✅\n",
            "Confidence: YTA: 0.897, NTA: 0.095\n",
            "\n",
            "--- Testing on custom examples ---\n",
            "\n",
            "Post: I told my pregnant sister that she can't bring her emotional support peacock to my wedding. She says I'm being unreasonable and that the peacock helps...\n",
            "Prediction: NTA\n",
            "\n",
            "Post: My neighbor's dog keeps pooping in my yard. After asking them multiple times to control their pet, I started collecting the poop and leaving it on the...\n",
            "Prediction: YTA\n",
            "\n",
            "Post: I ate my roommate's leftover pizza that had been in the fridge for a week. They got really mad even though it was starting to smell bad. I thought I w...\n",
            "Prediction: YTA\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Cell 12: Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")\n",
        "\n",
        "# Cell 13: Enable inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "print()\n",
        "\n",
        "# Cell 14: Remake the old lm_head\n",
        "# Save the current (trimmed) lm_head and bias\n",
        "trimmed_lm_head = model.lm_head.weight.data.clone()\n",
        "trimmed_lm_head_bias = model.lm_head.bias.data.clone() if hasattr(model.lm_head, \"bias\") and model.lm_head.bias is not None else torch.zeros(len(number_token_ids), device=trimmed_lm_head.device)\n",
        "\n",
        "# Create a new lm_head with shape [old_size, hidden_dim]\n",
        "hidden_dim = trimmed_lm_head.shape[1]\n",
        "new_lm_head = torch.full((old_size, hidden_dim), 0, dtype=trimmed_lm_head.dtype, device=trimmed_lm_head.device)\n",
        "new_lm_head_bias = torch.full((old_size,), -1000.0, dtype=trimmed_lm_head_bias.dtype, device=trimmed_lm_head_bias.device)\n",
        "\n",
        "# Fill in the weights and bias for the allowed tokens (number_token_ids)\n",
        "for new_idx, orig_token_id in enumerate(number_token_ids):\n",
        "    new_lm_head[orig_token_id] = trimmed_lm_head[new_idx]\n",
        "    new_lm_head_bias[orig_token_id] = trimmed_lm_head_bias[new_idx]\n",
        "\n",
        "# Update the model's lm_head weight and bias\n",
        "with torch.no_grad():\n",
        "    new_lm_head_module = torch.nn.Linear(hidden_dim, old_size, bias=True, device=model.device)\n",
        "    new_lm_head_module.weight.data.copy_(new_lm_head)\n",
        "    new_lm_head_module.bias.data.copy_(new_lm_head_bias)\n",
        "    model.lm_head.modules_to_save[\"default\"] = new_lm_head_module\n",
        "\n",
        "print(f\"Remade lm_head: shape = {model.lm_head.weight.shape}. Allowed tokens: {number_token_ids}\")\n",
        "\n",
        "# Cell 15: Batched inference on validation set\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Prepare inference prompt\n",
        "inference_prompt_template = prompt.split(\"class {}\")[0] + \"class \"\n",
        "\n",
        "# Sort validation set by length for efficient batching\n",
        "val_df['token_length'] = val_df['text'].apply(lambda x: len(tokenizer.encode(x, add_special_tokens=False)))\n",
        "val_df_sorted = val_df.sort_values(by='token_length').reset_index(drop=True)\n",
        "\n",
        "display = 20\n",
        "batch_size = 8  # Adjust based on your GPU\n",
        "device = model.device\n",
        "correct = 0\n",
        "results = []\n",
        "\n",
        "label_names = {1: 'YTA', 2: 'NTA'}\n",
        "\n",
        "# Calculate confusion matrix\n",
        "confusion_matrix = {'YTA_as_YTA': 0, 'YTA_as_NTA': 0, 'NTA_as_YTA': 0, 'NTA_as_NTA': 0}\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for i in tqdm(range(0, len(val_df_sorted), batch_size), desc=\"Evaluating\"):\n",
        "        batch = val_df_sorted.iloc[i:i+batch_size]\n",
        "        prompts = [inference_prompt_template.format(text) for text in batch['text']]\n",
        "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(device)\n",
        "        logits = model(**inputs).logits\n",
        "        last_idxs = inputs.attention_mask.sum(1) - 1\n",
        "        last_logits = logits[torch.arange(len(batch)), last_idxs, :]\n",
        "        probs_all = F.softmax(last_logits, dim=-1)\n",
        "        probs = probs_all[:, number_token_ids]\n",
        "        preds = torch.argmax(probs, dim=-1).cpu().numpy()\n",
        "\n",
        "        true_labels = batch['label'].tolist()\n",
        "        correct += sum([p == t for p, t in zip(preds, true_labels)])\n",
        "\n",
        "        # Update confusion matrix\n",
        "        for p, t in zip(preds, true_labels):\n",
        "            if t == 1 and p == 1:\n",
        "                confusion_matrix['YTA_as_YTA'] += 1\n",
        "            elif t == 1 and p == 2:\n",
        "                confusion_matrix['YTA_as_NTA'] += 1\n",
        "            elif t == 2 and p == 1:\n",
        "                confusion_matrix['NTA_as_YTA'] += 1\n",
        "            elif t == 2 and p == 2:\n",
        "                confusion_matrix['NTA_as_NTA'] += 1\n",
        "\n",
        "        for j in range(len(batch)):\n",
        "            results.append({\n",
        "                \"text\": batch['text'].iloc[j][:200],\n",
        "                \"true\": true_labels[j],\n",
        "                \"pred\": preds[j],\n",
        "                \"probs\": probs[j][1:].float().cpu().numpy(),\n",
        "                \"ok\": preds[j] == true_labels[j]\n",
        "            })\n",
        "\n",
        "accuracy = 100 * correct / len(val_df_sorted)\n",
        "print(f\"\\nValidation accuracy: {accuracy:.2f}% ({correct}/{len(val_df_sorted)})\")\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(f\"YTA predicted as YTA: {confusion_matrix['YTA_as_YTA']}\")\n",
        "print(f\"YTA predicted as NTA: {confusion_matrix['YTA_as_NTA']}\")\n",
        "print(f\"NTA predicted as YTA: {confusion_matrix['NTA_as_YTA']}\")\n",
        "print(f\"NTA predicted as NTA: {confusion_matrix['NTA_as_NTA']}\")\n",
        "\n",
        "print(\"\\n--- Random samples ---\")\n",
        "for s in random.sample(results, min(display, len(results))):\n",
        "    print(f\"\\nText: {s['text']}...\")\n",
        "    print(f\"True: {label_names[s['true']]}  Pred: {label_names[s['pred']]} {'✅' if s['ok'] else '❌'}\")\n",
        "    print(f\"Confidence: YTA: {s['probs'][0]:.3f}, NTA: {s['probs'][1]:.3f}\")\n",
        "\n",
        "# Clean up\n",
        "if 'token_length' in val_df:\n",
        "    del val_df['token_length']\n",
        "\n",
        "# Cell 17: Test on custom examples (optional)\n",
        "test_examples = [\n",
        "    \"I told my pregnant sister that she can't bring her emotional support peacock to my wedding. She says I'm being unreasonable and that the peacock helps with her anxiety. Now she's threatening not to come to the wedding at all.\",\n",
        "    \"My neighbor's dog keeps pooping in my yard. After asking them multiple times to control their pet, I started collecting the poop and leaving it on their doorstep. They called me childish and petty.\",\n",
        "    \"I ate my roommate's leftover pizza that had been in the fridge for a week. They got really mad even though it was starting to smell bad. I thought I was doing them a favor.\",\n",
        "]\n",
        "\n",
        "print(\"\\n--- Testing on custom examples ---\")\n",
        "for example in test_examples:\n",
        "    prompt_text = inference_prompt_template.format(example)\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\", truncation=True, max_length=max_seq_length).to(device)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=1, do_sample=False)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    prediction = response.split(\"class \")[-1].strip()\n",
        "    print(f\"\\nPost: {example[:150]}...\")\n",
        "    print(f\"Prediction: {label_names.get(int(prediction), 'Unknown')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h0YjtqGsyewg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "774051312b244179b362f2340d539a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67ad394be5e0493c986902fdca8067e3",
              "IPY_MODEL_cc0953e037c84e7d9ee7f0e1e3d3e489",
              "IPY_MODEL_2bb2e2caf0064da3bb5524b29ea00131"
            ],
            "layout": "IPY_MODEL_92e892581ded4cf187e6638012ec68ba"
          }
        },
        "67ad394be5e0493c986902fdca8067e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b18841d47e748b4be8384f56495ba4b",
            "placeholder": "​",
            "style": "IPY_MODEL_d13c1b1a93a1455bb42dd6c244112c6d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "cc0953e037c84e7d9ee7f0e1e3d3e489": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_980716e0a3c04bd3943519aff7bb424f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fce6e3190444a8094720022a6d310a7",
            "value": 2
          }
        },
        "2bb2e2caf0064da3bb5524b29ea00131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7db5094af99f48e696a4bb7d06cb291c",
            "placeholder": "​",
            "style": "IPY_MODEL_93add42250c944259857ee87d465a693",
            "value": " 2/2 [00:02&lt;00:00,  1.20s/it]"
          }
        },
        "92e892581ded4cf187e6638012ec68ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b18841d47e748b4be8384f56495ba4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d13c1b1a93a1455bb42dd6c244112c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "980716e0a3c04bd3943519aff7bb424f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fce6e3190444a8094720022a6d310a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7db5094af99f48e696a4bb7d06cb291c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93add42250c944259857ee87d465a693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d095bc2d571428d97f832ef17a1a4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9156df23de8d4f3c84b171b58be4cba5",
              "IPY_MODEL_1b6eb6008025446182410b733646f198",
              "IPY_MODEL_456b458a44f14d82a50b4c452239865b"
            ],
            "layout": "IPY_MODEL_e474e22744ad4f8db379078dee7d1ff8"
          }
        },
        "9156df23de8d4f3c84b171b58be4cba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56e29dbc7e704b38980517c6834e7e91",
            "placeholder": "​",
            "style": "IPY_MODEL_8e33c2d8e87b43a4b597c9d5b56aa782",
            "value": "Unsloth: Tokenizing [&quot;text&quot;]: 100%"
          }
        },
        "1b6eb6008025446182410b733646f198": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bae24268b2a477dbb7fd6d72cea2c8f",
            "max": 1454,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f02d2ab38b2040ba985f4d6faaed09e6",
            "value": 1454
          }
        },
        "456b458a44f14d82a50b4c452239865b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbf255a5e1874a78aa892a5d18f9ce1e",
            "placeholder": "​",
            "style": "IPY_MODEL_8e247d05994f4cd7a2aee50de9d4d1d3",
            "value": " 1454/1454 [00:00&lt;00:00, 2501.14 examples/s]"
          }
        },
        "e474e22744ad4f8db379078dee7d1ff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e29dbc7e704b38980517c6834e7e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e33c2d8e87b43a4b597c9d5b56aa782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bae24268b2a477dbb7fd6d72cea2c8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f02d2ab38b2040ba985f4d6faaed09e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bbf255a5e1874a78aa892a5d18f9ce1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e247d05994f4cd7a2aee50de9d4d1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "state": {}
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
